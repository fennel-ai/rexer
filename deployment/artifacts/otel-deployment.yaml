---
apiVersion: v1
kind: Namespace
metadata:
  name: otel-eks
  labels:
    name: otel-eks
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: otel-collector-account
  namespace: otel-eks
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: aoc-agent-role
rules:
  - apiGroups:
      - ""
    resources:
      - nodes
      - nodes/proxy
      - nodes/metrics
      - services
      - endpoints
      - pods
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - nodes/stats
      - configmaps
      - events
    verbs:
      - create
      - get
  # To ensure that only one ADOT collector from the DaemonSet is collecting cluster-level metrics
  - apiGroups:
      - ""
    resources:
      - configmaps
    resourceNames:
      - otel-container-insight-clusterleader
    verbs:
      - get
      - update
  - apiGroups:
      - extensions
    resources:
      - ingresses
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - apps
    resources:
      - replicasets
    verbs:
      - list
      - watch
  - apiGroups:
      - batch
    resources:
      - jobs
    verbs:
      - list
      - watch
  - nonResourceURLs:
      - /metrics
    verbs:
      - get
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: aoc-agent-role-binding
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: aoc-agent-role
subjects:
  - kind: ServiceAccount
    name: otel-collector-account
    namespace: otel-eks
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: otel-agent-conf
  namespace: otel-eks
  labels:
    app: opentelemetry
    component: otel-agent-conf
data:
  otel-agent-config: |
    extensions:
      health_check:
      pprof:
      zpages:

    receivers:
      otlp:
        protocols:
          grpc:
            endpoint: 0.0.0.0:4317
          http:
            endpoint: 0.0.0.0:4318
      awscontainerinsightreceiver:
      prometheus:
        config:
          global:
            scrape_interval: 1m
            scrape_timeout: 10s
          scrape_configs:
            - job_name: 'kubernetes-pods'
              sample_limit: 10000
              kubernetes_sd_configs:
              - role: pod
              relabel_configs:
              - action: keep
                regex: true
                source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
              - action: replace
                source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
                regex: ([^:]+)(?::\d+)?;(\d+)
                replacement: $$1:$$2
                target_label: __address__
              - action: replace
                source_labels: [__meta_kubernetes_namespace]
                target_label: Namespace
              - action: replace
                source_labels: [__meta_kubernetes_pod_name]
                target_label: PodName
              - action: drop
                source_labels: [__meta_kubernetes_pod_container_name]
                regex: (linkerd-init|linkerd-proxy)
              - action: replace
                source_labels: [__meta_kubernetes_pod_container_name]
                target_label: ContainerName
              - action: replace
                source_labels: [__meta_kubernetes_pod_controller_name]
                target_label: PodControllerName
              - action: replace
                source_labels: [__meta_kubernetes_pod_controller_kind]
                target_label: PodControllerKind
              - action: replace
                source_labels: [__meta_kubernetes_pod_phase]
                target_label: PodPhase
              # Exclude high cardinality metrics
              metric_relabel_configs:
              - action: drop
                source_labels: [__name__]
                regex: 'go_gc_duration_seconds.*'
    
            - job_name: 'kubernetes-service'
              kubernetes_sd_configs:
              - role: service
              relabel_configs:
              - action: replace
                source_labels: [__meta_kubernetes_namespace]
                target_label: Namespace
              - action: replace
                source_labels: [__meta_kubernetes_service_name]
                target_label: Service

            - job_name: 'kubernetes-cadvisor'
              sample_limit: 10000
              # Default to scraping over https. If required, just disable this or change to
              # `http`.
              scheme: https
              metrics_path: /metrics/cadvisor
              # overwrites the global configuration
              scrape_interval: 10s
              
              kubernetes_sd_configs:
              - role: node
              tls_config:
                ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
                insecure_skip_verify: true
              bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
              
              relabel_configs:
              - action: labelmap
                regex: __meta_kubernetes_node_label_(.+)

    processors:
      # increase the timeout (time after which a batch is sent regardless), defaults to 200ms to maximizing
      # the number of spans sent collectively to the destination (cloudwatch xray).
      # rest of the values are set to default `end_batch_size (default = 8192)` and `send_batch_max_size (default = 0)`
      batch/traces:
        timeout: 10s
      batch/metrics:
        timeout: 60s
      resourcedetection:
        detectors:
          - env
          - system
          - ec2
          - eks
        timeout: 10s
        override: false
      resource:
        attributes:
          - key: TaskId
            from_attribute: job
            action: insert
          - key: receiver
            value: "prometheus"
            action: insert

    exporters:
      awsxray:
        region: "%%AWS_REGION%%"
      awsemf/containerinsights:
        namespace: ContainerInsights
        log_group_name: '/aws/containerinsights/%%PLANE_ID%%/{ClusterName}/performance'
        log_stream_name: '{NodeName}'
        resource_to_telemetry_conversion:
          enabled: true
        dimension_rollup_option: NoDimensionRollup
        parse_json_encoded_attr_values: [Sources, kubernetes]
        metric_declarations:
          # node metrics
          - metric_name_selectors:
              - node_cpu_utilization
              - node_memory_utilization
              - node_cpu_reserved_capacity
              - node_memory_reserved_capacity
            dimensions: [[NodeName, InstanceId, ClusterName]]
          - metric_name_selectors:
              - node_cpu_usage_total
              - node_cpu_limit
              - node_memory_working_set
              - node_memory_limit
            dimensions: [[ClusterName]]
          # pod metrics
          - metric_name_selectors:
              - pod_cpu_utilization
              - pod_memory_utilization
              - pod_cpu_utilization_over_pod_limit
              - pod_memory_utilization_over_pod_limit
              - pod_cpu_reserved_capacity
              - pod_memory_reserved_capacity
              - pod_number_of_container_restarts
            dimensions: [[PodName, Namespace, ClusterName]]
          # cluster metrics
          - metric_name_selectors:
              - cluster_node_count
              - cluster_failed_node_count
            dimensions: [[ClusterName]]

      awsprometheusremotewrite:
        # This should be replaced before it is used for deployment
        endpoint: "%%AMP_ENDPOINT%%"
        aws_auth:
          service: "aps"
          region: "%%AWS_REGION%%"

    service:
      telemetry:
        logs:
          level: "debug"
      extensions:
        - health_check
        - pprof
        - zpages
      pipelines:
        traces:
          receivers:
            - otlp
          processors:
            - batch/traces
          exporters:
            - awsxray
        metrics/containerinsights:
          receivers:
            - awscontainerinsightreceiver
          processors:
            - resourcedetection
            - resource
            - batch/metrics
          exporters:
            - awsemf/containerinsights
        metrics/prometheus:
          receivers:
            - prometheus
          processors:
            - resourcedetection
            - resource
            - batch/metrics
          exporters:
            - awsprometheusremotewrite

---
# create standalone deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: otel-collector
  namespace: otel-eks
spec:
  replicas: 1
  selector:
    matchLabels:
      name: otel-collector
  template:
    metadata:
      labels:
        name: otel-collector
    spec:
      serviceAccountName: otel-collector-account
      # otel collector do not publish arm64 docker images but build arm64 binaries
      #
      # TODO(mohit): we can tag arm64; `latest` -> `0.54.0-arm64`
      # Or wait for https://github.com/open-telemetry/opentelemetry-collector-contrib/issues/2379 to be fixed
      nodeSelector:
        kubernetes.io/arch: "amd64"
      containers:
        - name: otel-collector
          image: otel/opentelemetry-collector-contrib:latest
          env:
            - name: K8S_NODE_NAME
              valueFrom:
                fieldRef:
                  fieldPath: spec.nodeName
            - name: HOST_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.hostIP
            - name: HOST_NAME
              valueFrom:
                fieldRef:
                  fieldPath: spec.nodeName
            - name: K8S_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
          imagePullPolicy: Always
          ports:
            - name: otlp
              containerPort: 4317
              protocol: TCP
            - name: otlp-http
              containerPort: 4318
              protocol: TCP
          command:
            - "/otelcontribcol"
            - "--config=/conf/otel-agent-config.yaml"
          volumeMounts:
            - name: rootfs
              mountPath: /rootfs
              readOnly: true
            - name: dockersock
              mountPath: /var/run/docker.sock
              readOnly: true
            - name: varlibdocker
              mountPath: /var/lib/docker
              readOnly: true
            - name: sys
              mountPath: /sys
              readOnly: true
            - name: devdisk
              mountPath: /dev/disk
              readOnly: true
            - name: otel-agent-config-vol
              mountPath: /conf
          resources:
            limits:
              cpu: 512m
              memory: 512Mi
            requests:
              cpu: 128m
              memory: 128Mi
      volumes:
        - configMap:
            name: otel-agent-conf
            items:
              - key: otel-agent-config
                path: otel-agent-config.yaml
          name: otel-agent-config-vol
        - name: rootfs
          hostPath:
            path: /
        - name: dockersock
          hostPath:
            path: /var/run/docker.sock
        - name: varlibdocker
          hostPath:
            path: /var/lib/docker
        - name: sys
          hostPath:
            path: /sys
        - name: devdisk
          hostPath:
            path: /dev/disk/
---
# Source: opentelemetry-collector/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: otel-collector
  namespace: otel-eks
spec:
  type: ClusterIP
  ports:
    - name: otlp
      port: 4317
      targetPort: otlp
      protocol: TCP
    - name: otlp-http
      port: 4318
      targetPort: otlp-http
      protocol: TCP
  selector:
    name: otel-collector
---